{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.signal import savgol_filter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and merge the two datasets\n",
    "data1 = pd.read_csv('data_part_1.csv')\n",
    "data2 = pd.read_csv('data_part_2.csv')\n",
    "\n",
    "# Get common columns between the two datasets\n",
    "common_columns = list(set(data1.columns) & set(data2.columns))\n",
    "common_columns = [col for col in data1.columns if col in common_columns]\n",
    "\n",
    "data1_aligned = data1[common_columns]\n",
    "data2_aligned = data2[common_columns]\n",
    "\n",
    "merged_data = pd.concat([data1_aligned, data2_aligned], ignore_index=True)\n",
    "data = merged_data.drop(merged_data.columns[0], axis=1)\n",
    "\n",
    "# Check the shape of the merged dataset\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Division of data into calibration, validation and test partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data.iloc[:, :20] \n",
    "X = data.iloc[:, 20:]  \n",
    "X_bands = X.columns.values.astype('int')\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data centering and scaling techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training set and transform the data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Apply the same transformation to the validation and test sets\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessment of outliers and missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = X.isnull().sum()\n",
    "\n",
    "# Filter out the features that have missing values\n",
    "missing_features = missing_values[missing_values > 0] \n",
    "\n",
    "print(missing_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z-scores\n",
    "z_scores = np.abs(stats.zscore(X_train_scaled))\n",
    "outliers = np.where(z_scores > 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique indices of outlier rows (samples)\n",
    "outlier_rows = np.unique(outliers[0])\n",
    "\n",
    "# Randomly select 10 outlier samples\n",
    "selected_outlier_samples = np.random.choice(outlier_rows, size=10, replace=False)\n",
    "\n",
    "# Plot these samples and highlight their outliers\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "for i, sample_idx in enumerate(selected_outlier_samples):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    \n",
    "    # Plot all feature values for the sample\n",
    "    plt.scatter(X_bands, X_train_scaled[sample_idx, :], alpha=0.6)\n",
    "    \n",
    "    # Highlight outlier values in red\n",
    "    sample_outlier_cols = outliers[1][outliers[0] == sample_idx]\n",
    "    plt.scatter(X_bands[sample_outlier_cols], X_train_scaled[sample_idx, sample_outlier_cols], color='red')\n",
    "\n",
    "    plt.title(f\"Sample {sample_idx}\")\n",
    "    plt.xlabel('Feature Index')\n",
    "    plt.ylabel('Scaled Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The z-score shows that the outliers are continuous and have nothing in common in each sample, considering that there may be some samples that are special cases that are not treated as outliers first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Savitzky-Golay filter to smooth the spectrum data for each sample\n",
    "# Set window size and polynomial order\n",
    "window_size = 45  \n",
    "poly_order = 3\n",
    "\n",
    "X_train_smoothed = savgol_filter(X_train_scaled, window_length=window_size, polyorder=poly_order, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Referring to the vertical jumps and possible outliers that have appeared in the previous spectral curves, try smoothed with Savitzky-Golay filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compare before and after smoothing curve\n",
    "\n",
    "# Randomly select 6 sample indices\n",
    "sample_indices = np.random.choice(X_train_scaled.shape[0], 6, replace=False)\n",
    "\n",
    "# Plot comparison of original and smoothed data\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i, sample_idx in enumerate(sample_indices, 1):\n",
    "    plt.subplot(3, 2, i)\n",
    "    plt.plot(X_bands, X_train_scaled[sample_idx, :], label='Original', alpha=0.6)\n",
    "    plt.plot(X_bands, X_train_smoothed[sample_idx, :], label='Smoothed', alpha=0.8, linestyle='--')\n",
    "    plt.title(f'Sample {sample_idx}: Original vs Smoothed Spectrum')\n",
    "    plt.xlabel('Wavelength Index')\n",
    "    plt.ylabel('Intensity')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(hspace=0.4)  # Add extra space between subplots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Pretreated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data_counts = y_train.notna().sum()\n",
    "top_5_features = valid_data_counts.nlargest(5).index\n",
    "\n",
    "print(\"Top 5 target features based on valid data counts:\", top_5_features)\n",
    "\n",
    "y_train_top_5 = y_train[top_5_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target feature with the largest amount of data is selected as the subsequent prediction target. Save these five features and the corresponding spectral data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store X and y for each target feature, only including samples without missing values\n",
    "data_storage = {}\n",
    "\n",
    "# Iterate through each target feature\n",
    "for target in y_train_top_5.columns:\n",
    "    valid_indices = y_train_top_5[target].notna()\n",
    "    \n",
    "    # Extract the corresponding X and y, excluding samples with missing values\n",
    "    X_valid = X_train_smoothed[valid_indices]\n",
    "    y_valid = y_train_top_5[target][valid_indices]\n",
    "    \n",
    "    data_storage[target] = {\n",
    "        'X': X_valid,  \n",
    "        'y': y_valid   \n",
    "    }\n",
    "    \n",
    "    X_shape = X_valid.shape\n",
    "    y_shape = y_valid.shape\n",
    "    print(f\"Target: {target} -> X shape: {X_shape}, y shape: {y_shape}\")\n",
    "\n",
    "# Output the data storage structure\n",
    "print(f\"Data storage keys: {list(data_storage.keys())}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target, data in data_storage.items():\n",
    "    \n",
    "    # Perform PCA\n",
    "    pca = PCA()\n",
    "    X_pca = pca.fit_transform(X)  # Data after dimensionality reduction\n",
    "    \n",
    "    explained_variance_ratio = pca.explained_variance_ratio_\n",
    "    \n",
    "    # Visualize the explained variance ratio\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio, alpha=0.6)\n",
    "    plt.step(range(1, len(explained_variance_ratio) + 1), np.cumsum(explained_variance_ratio), where='mid', label='Cumulative Explained Variance')\n",
    "    plt.xlabel('Principal Components')\n",
    "    plt.ylabel('Explained Variance Ratio')\n",
    "    plt.title(f'Explained Variance by PCs for {target}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Print cumulative explained variance\n",
    "    cumulative_variance = np.cumsum(explained_variance_ratio)\n",
    "    print(f\"Target: {target} -> Explained variance (first few components): {explained_variance_ratio[:5]}\")\n",
    "    print(f\"Target: {target} -> Cumulative explained variance (first few components): {cumulative_variance[:5]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
